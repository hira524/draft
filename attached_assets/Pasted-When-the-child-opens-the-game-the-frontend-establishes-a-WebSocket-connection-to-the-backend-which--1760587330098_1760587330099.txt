When the child opens the game, the frontend establishes a WebSocket connection to the backend which fetches the child's interests from the database and uses OpenAI to generate a personalized word list, then the AI greets the child via Deepgram TTS streaming audio chunks through the WebSocket while setting a botIsSpeaking flag, and when the greeting finishes, the AI introduces the first word with pronunciation guidance, then sets waitingForChildResponse flag and the frontend continuously captures microphone audio in 20ms chunks streaming to the backend which forwards to Deepgram STT, and when a final transcript arrives, the backend checks that the bot isn't speaking, then sets botIsBusy flag, analyzes the pronunciation by comparing the transcript to the target word using phonetic encoding and Deepgram's confidence score to calculate an accuracy percentage, then sends this analysis to OpenAI which generates contextual feedback based on whether it was correct (award points and move to next word), partially correct (provide specific guidance and ask to try again), or the third failed attempt (give encouragement and move to next word), then this feedback text goes to Deepgram TTS which returns audio that's chunked and streamed back through the WebSocket while setting botIsSpeaking flag, and this cycle repeats continuously for each word until the game ends, with all state flags carefully managed to prevent the AI from listening while speaking and to ensure smooth turn-taking in the conversation.

Create a complete real-time AI pronunciation tutor game for children that works exactly like ChatGPT's Voice Mode, using continuous streaming audio conversation powered by Deepgram STT/TTS and OpenAI.

CORE ARCHITECTURE:
Build a WebSocket-based system where the child and AI engage in seamless, real-time voice conversation. The AI acts as a friendly pronunciation tutor who continuously listens, evaluates pronunciation, provides instant feedback, and guides the child through a personalized word practice game.

TECHNICAL STACK:
- Backend: Node.js with TypeScript
- Frontend: React with TypeScript
- STT: Deepgram streaming Speech-to-Text API (real-time transcription)
- TTS: Deepgram streaming Text-to-Speech API (real-time audio generation)
- AI: OpenAI GPT-4 API for intelligent conversation and feedback generation
- WebSocket: For continuous bidirectional audio streaming
- Database: PostgreSQL/MongoDB for storing child profiles and interests
- Audio Processing: Web Audio API for microphone capture

GAME FLOW LOGIC:

1. INITIALIZATION PHASE:
   - Child logs in, system fetches their interests from database (e.g., "animals", "space", "dinosaurs")
   - Backend calls OpenAI to dynamically generate 15-20 age-appropriate words based on interests
   - Prompt: "Generate 20 pronunciation practice words for a [age]-year-old child interested in [interests]. Start with easy 3-4 letter words, progress to medium difficulty. Return as JSON: [{word, difficulty, phonetic, hint}]"
   - WebSocket establishes persistent connection between frontend and backend
   - AI sends greeting via TTS: "Hi [child's name]! I'm your pronunciation buddy! Today we'll practice fun words about [interests]. Ready to start?"

2. WORD INTRODUCTION PHASE:
   - AI introduces current word with detailed pronunciation guidance
   - Example: "Let's practice the word 'STAR'. Listen: STAR. The 'st' sound is made by pressing your tongue to the top of your mouth, then open wide for 'ar' like saying 'ah'. Now you try!"
   - Backend sets state flags: botIsSpeaking=true, then after TTS completes: botIsSpeaking=false, waitingForChildResponse=true
   - Frontend displays target word, visual mouth position hints, and microphone indicator

3. CONTINUOUS LISTENING PHASE:
   - Frontend captures microphone audio in 20ms chunks (PCM/mulaw format at 16kHz or 8kHz)
   - Audio chunks stream continuously through WebSocket to backend
   - Backend forwards audio stream to Deepgram STT (streaming mode with settings: model='nova-2', language='en-US', interim_results=true, endpointing=300, encoding='linear16' or 'mulaw', sample_rate=16000)
   - Deepgram returns interim transcripts (display but don't process) and final transcripts (process these)
   - BARGE-IN PROTECTION: If botIsSpeaking=true, ignore all transcripts to prevent interruption

4. PRONUNCIATION ANALYSIS PHASE (Critical):
   When final transcript received from Deepgram:
   
   Step A - Validation:
   - Check botIsSpeaking == false (AI must be silent)
   - Check waitingForChildResponse == true (we're expecting child's attempt)
   - Set botIsBusy = true (prevent concurrent processing)
   
   Step B - Pronunciation Scoring:
   - Extract transcript text (what child said)
   - Get Deepgram confidence score (0-1 scale)
   - Perform phonetic analysis:
     * Install libraries: metaphone, natural, string-similarity, or write custom phoneme matcher
     * Convert target word to phonetic encoding: "star" â†’ IPA /stÉ‘r/ or Metaphone "STAR"
     * Convert child's speech to phonetic: "stor" â†’ IPA /stÉ”r/ or Metaphone "STR"
     * Calculate Levenshtein distance (character-level similarity)
     * Calculate phoneme-level similarity (compare phonetic representations)
   
   - Scoring Formula:
     ```
     baseScore = (phonemeSimilarity * 0.6) + (deepgramConfidence * 0.4)
     
     If transcript.toLowerCase() === targetWord.toLowerCase() && confidence > 0.85:
       finalScore = 100% (perfect match)
     Else if baseScore >= 0.80:
       finalScore = 80-99% (correct with minor accent variation)
     Else if baseScore >= 0.60:
       finalScore = 60-79% (partially correct, needs guidance)
     Else:
       finalScore = 0-59% (incorrect pronunciation)
     ```
   
   Step C - Identify Specific Errors:
   - Compare phonemes to find which sounds were wrong
   - Example: Target "star" vs Child "stor" â†’ vowel error: "o" instead of "a"
   - Store error details for AI feedback generation
   
   Step D - Determine Outcome:

ðŸŽ¯ Complete AI Pronunciation Game Implementation Prompt
if (finalScore >= 80) {
success = true;
awardPoints(10);
attemptCount = 0; // reset for next word
} else {
success = false;
attemptCount++;
if (attemptCount >= 3) {
// Move to next word after 3 attempts
maxAttemptsReached = true;
}
}


5. AI FEEDBACK GENERATION PHASE:
- Send analysis results to OpenAI to generate contextual, encouraging feedback
- System Prompt: "You are a warm, encouraging pronunciation tutor for young children. Provide clear, simple feedback using child-friendly language. Always be positive and motivating."

- User Message Format:
```json
{
  "targetWord": "star",
  "childSaid": "stor",
  "pronunciationScore": 72,
  "attemptNumber": 1,
  "maxAttempts": 3,
  "phonemeErrors": ["vowel 'a' pronounced as 'o'"],
  "deepgramConfidence": 0.85,
  "childName": "Emma",
  "currentPoints": 50
}

OpenAI Response Examples:
Success (score >= 80): "Wonderful! You said 'star' perfectly! That was amazing! You earned 10 points! Your total is now 60 points. Ready for the next word?"
Partial (attempt 1-2, score 60-79): "Good try! You said 'stor' but the word is 'star'. Try to open your mouth wider and say 'ah' for the middle sound: ST-AH-R. Let's try once more!"
Low (attempt 1-2, score < 60): "Let's practice together! The word is 'star'. Break it down: 'st' like a snake sound, then 'ar' like you're at the doctor saying 'ahhh'. Listen again: STAR. Now you try!"
Failed (attempt 3): "You gave it a great effort! The word was 'star'. Don't worry, we'll practice it again later. Let's try a new word!"
TTS RESPONSE PHASE:

Send OpenAI feedback text to Deepgram TTS
TTS Settings: model='aura-luna-en' (fast, child-friendly), encoding='linear16' or 'mulaw', sample_rate=16000 or 8000, speed=1.0
Receive audio buffer from Deepgram
Chunk audio into 20ms frames (320 bytes for 16kHz linear16, 160 bytes for 8kHz mulaw)
Set botIsSpeaking = true
Stream audio chunks through WebSocket to frontend every 15-20ms
Frontend plays audio chunks in real-time using Web Audio API
When last chunk sent, send end marker: { event: 'audio_end' }
Set botIsSpeaking = false, botIsBusy = false
PROGRESSION LOGIC:
if (success) {
  // Move to next word
  currentWordIndex++;
  attemptCount = 0;
  if (currentWordIndex >= wordList.length) {
    // Game complete
    sendGameSummary();
  } else {
    // Start next word (go to step 2)
    introduceNextWord();
  }
} else if (attemptCount < 3) {
  // Retry same word (stay in listening mode, step 3)
  waitingForChildResponse = true;
} else {
  // Max attempts reached, move to next word
  currentWordIndex++;
  attemptCount = 0;
  introduceNextWord();
}

// Conversation state
let botIsSpeaking = false;          // AI is currently outputting TTS
let botIsBusy = false;               // AI is processing/analyzing (prevent concurrent requests)
let waitingForChildResponse = false; // System expects child to speak target word
let sttReady = false;                // Deepgram STT connection established

// Game state
let currentWord = "";                // Current target word
let wordList = [];                   // Array of words for this session
let currentWordIndex = 0;            // Progress through word list
let attemptCount = 0;                // Attempts for current word (0-3)
let totalScore = 0;                  // Total points earned
let wordsCompleted = 0;              // Successfully pronounced words
let sessionId = "";                  // Unique session identifier

// Audio state
let audioChunksBuffer = [];          // Buffer for incoming audio chunks
let lastTranscriptTime = 0;          // Throttle duplicate transcripts


FRONTEND IMPLEMENTATION:

File: src/components/PronunciationGame.tsx

Main game interface with visual feedback
Microphone permission handling
Web Audio API setup for capturing audio
WebSocket connection management
Real-time UI updates (points, current word, attempt counter, feedback display)
Visual indicators: microphone active, AI speaking, processing
Animation for correct/incorrect responses
Progress bar for word completion
File: src/services/AudioManager.ts

Capture microphone audio using getUserMedia()
Convert to appropriate format (PCM 16-bit or mulaw 8-bit at 16kHz or 8kHz)
Chunk audio into 20ms segments
Send chunks via WebSocket
Receive audio chunks from WebSocket
Play audio using Web Audio API AudioContext
File: src/services/WebSocketService.ts

Establish WebSocket connection to backend
Handle connection events (open, close, error, reconnect)
Send audio chunks
Receive STT transcripts, TTS audio, game state updates
Event emitter for UI updates
BACKEND IMPLEMENTATION:

File: src/server.ts

Express server setup
WebSocket server (ws library)
Handle WebSocket connections
Route audio to appropriate handlers
Manage session state per connection
File: src/services/DeepgramService.ts

Initialize Deepgram client with API key
Setup streaming STT connection with optimal settings
Forward audio chunks to Deepgram
Handle transcript events (interim and final)
Setup TTS synthesis
Return audio buffers
File: src/services/PronunciationAnalyzer.ts

Implement phonetic comparison logic
Calculate pronunciation scores
Identify specific phoneme errors
Return structured analysis results
File: src/services/OpenAIService.ts

Initialize OpenAI client
Generate dynamic word lists based on child interests
Generate contextual feedback based on pronunciation analysis
Maintain conversation history for context
File: src/services/WordGenerator.ts

Fetch child profile and interests from database
Call OpenAI to generate personalized word list
Cache word lists per session
Track word progress
File: src/services/GameStateManager.ts

Manage game state machine
Track current word, attempts, scores
Handle progression logic
Validate state transitions
Prevent invalid states
File: src/services/DatabaseService.ts

Connect to database
Fetch child profile (name, age, interests, skill level)
Save game sessions and results
Track progress over time
AUDIO STREAMING SPECIFICATIONS:

Format: Linear16 PCM or mulaw
Sample Rate: 16kHz (recommended) or 8kHz
Chunk Size: 20ms (320 bytes for 16kHz linear16, 160 bytes for 8kHz mulaw)
Channels: 1 (mono)
Bit Depth: 16-bit for linear16, 8-bit for mulaw
Streaming: Continuous, no gaps
Latency Target: < 500ms end-to-end (speech to response)
ERROR HANDLING & EDGE CASES:

WebSocket disconnection: Auto-reconnect with exponential backoff
Deepgram API failures: Retry with fallback models
OpenAI API failures: Use cached generic responses
Background noise: Filter with voice activity detection (VAD)
Child says nothing: Timeout after 10 seconds, AI prompts gently
Child says wrong word entirely: Acknowledge and guide back to target word
Multiple children speaking: Detect and ask one at a time
Audio device changes: Re-initialize microphone
Network latency: Buffer management and adaptive streaming
UI/UX REQUIREMENTS:

Clean, colorful, child-friendly design
Large, readable fonts
Visual feedback for microphone (animated when listening)
Loading indicators when AI is processing
Confetti/celebration animation for correct answers
Gentle shake animation for incorrect attempts
Progress bar showing word completion
Current points displayed prominently
Attempt counter (dots or hearts)
"AI is speaking" indicator to prevent interruption
Pronunciation tips with visual mouth diagrams
TESTING REQUIREMENTS:

Unit tests for pronunciation scoring algorithm
Integration tests for WebSocket flow
End-to-end tests for complete game session
Test with various accents and pronunciations
Test edge cases (silence, background noise, wrong words)
Performance testing (latency measurement)
Load testing (multiple concurrent sessions)
CONFIGURATION (.env):
DEEPGRAM_API_KEY=your_deepgram_key
OPENAI_API_KEY=your_openai_key
DATABASE_URL=your_database_connection_string
PORT=3000
WS_PORT=8080
NODE_ENV=development
LOG_LEVEL=debug
SESSION_TIMEOUT_MS=1800000
MAX_ATTEMPTS_PER_WORD=3
POINTS_PER_CORRECT_WORD=10
STT_MODEL=nova-2
TTS_MODEL=aura-luna-en
SAMPLE_RATE=16000
AUDIO_ENCODING=linear16

DELIVERABLES:

Complete working backend (TypeScript/Node.js)
Complete working frontend (React/TypeScript)
Database schema and migration scripts
Detailed README with setup instructions
API documentation
Architecture diagrams
Sample .env file
Docker configuration (optional)
Deployment guide
OPTIMIZATION REQUIREMENTS:

Minimize latency at every step (target < 500ms total)
Use streaming APIs, never batch processing
Implement efficient buffering strategies
Optimize audio chunk size for balance between latency and reliability
Cache OpenAI word lists per interest category
Use connection pooling for database
Implement rate limiting and throttling
Monitor and log performance metrics
The system should feel natural and conversational, like the child is practicing with a patient, encouraging human tutor who responds immediately and keeps the learning experience engaging and fun through continuous real-time voice interaction.


